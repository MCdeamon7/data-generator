{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import math\n",
    "import concurrent.futures\n",
    "from multiprocessing import Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almost all configurable variables are kept here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#day of weeks\n",
    "#Monday =0 .. Sunday=6\n",
    "work_days=range(5)\n",
    "work_hours = range(8,18)\n",
    "\n",
    "start_working_time=8\n",
    "end_working_time=17\n",
    "#down period is in seconds\n",
    "down_for_all_day=False\n",
    "#up period is in seconds\n",
    "up_period=3600\n",
    "\n",
    "mac_list=[\"F6:B8:15:1E:A8:02\",\n",
    "            \"6C:D0:7E:63:10:F1\",\n",
    "            \"97:46:62:3B:15:2C\",\n",
    "            \"05:A6:2E:30:98:E2\"]\n",
    "#one flag for each mac\n",
    "plc_state=[False,False,False,False]\n",
    "pls_abnormal_state=[False,False,False,False]\n",
    "plc_counter=[0,0,0,0]\n",
    "max_cycle_time=[41,48,34,74]\n",
    "\n",
    "\n",
    "#mu is the mean\n",
    "mu_cycle_time=[24,27,21,38]\n",
    "mu_ram=[7464,8197,6465,9883]\n",
    "mu_packets=20\n",
    "mu_packets_size=50\n",
    "#sigma is the standard deviation\n",
    "sigma_cycle_time=4\n",
    "sigma_ram=256\n",
    "sigma_packets=3\n",
    "sigma_packets_size=5\n",
    "\n",
    "#from how many days ago should i start\n",
    "back_in_time = 365\n",
    "\n",
    "packets_nmap_probability=0.01\n",
    "packets_big_probability=0.01\n",
    "#probabilità che i sistemi siano spenti durante un giorno normale\n",
    "random_down_for_all_day_probability=0.01\n",
    "#probabilità che i sistemi siano accesi un ora durante un giorno di shutdown\n",
    "random_up_probability=0.01\n",
    "\n",
    "#window_check must be in seconds and \n",
    "new_day_check_time = datetime(1,1,1,0,0,0,0)\n",
    "\n",
    "day_start_time = datetime(1,1,1,0,0,0)\n",
    "day_stop_time = datetime(1,1,1,0,0,0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## variables regarding the temporal space to generate the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_to_go_back = 365\n",
    "#frequency = '1S'   #every second\n",
    "frequency = '1T'    #every minute\n",
    "#frequency = '1H'    #every hour\n",
    "\n",
    "start_date = (datetime.today() - timedelta(days = days_to_go_back)).replace(hour=0,minute=0,second=0, microsecond=0)\n",
    "stop_date = datetime.today().replace(hour=0,minute=0,second=0, microsecond=0)\n",
    "\n",
    "#datelist = pd.date_range(datetime.today() - timedelta(days = back_in_time), datetime.today(), freq='S').tolist()\n",
    "datelist = pd.date_range(start_date, stop_date, freq=frequency).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525601\n",
      "2022-11-28 00:01:00\n"
     ]
    }
   ],
   "source": [
    "# how many rows are created\n",
    "print(len(datelist))\n",
    "#to check that the first line is actually spaced out from 00:00:00 the time you chose\n",
    "print(datelist[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to simplify the code down and making easier to mantain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cpu_cycle_time(date, mac_index):\n",
    "    return math.floor(random.gauss(mu=mu_cycle_time[mac_index],sigma=sigma_cycle_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ram_usage(date, mac_index):\n",
    "    return math.floor(random.gauss(mu=mu_ram[mac_index],sigma=sigma_ram))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ping packets are 56 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rx_size(packet_n, date, mac_index):\n",
    "    result = 0\n",
    "    for i in range(packet_n):\n",
    "        result += random.gauss(mu=mu_packets_size,sigma=sigma_packets_size)\n",
    "    return math.floor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tx_size(packet_n, date, mac_index):\n",
    "    result = 0\n",
    "    for i in range(packet_n):\n",
    "        result += random.gauss(mu=mu_packets_size,sigma=sigma_packets_size)\n",
    "    return math.floor(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## randomize the starting time with a margin of 15 minnutes, to make shure the data are not too predictable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: refactoring with variables and not static integers\n",
    "\n",
    "def randomize_start_and_stop():\n",
    "    global day_start_time\n",
    "    global day_stop_time\n",
    "\n",
    "    day_start_time = day_start_time.replace(hour=random.randint(7,8))\n",
    "    if (day_start_time.hour == 8):\n",
    "        day_start_time = day_start_time.replace(minute=random.randint(0,15))\n",
    "    else:\n",
    "        day_start_time = day_start_time.replace(minute=59-random.randint(0,15))\n",
    "\n",
    "    day_stop_time = day_start_time.replace(hour=random.randint(16,17))\n",
    "    if (day_stop_time.hour == 17):\n",
    "        day_stop_time = day_stop_time.replace(minute=random.randint(0,15))\n",
    "    else:\n",
    "        day_stop_time = day_stop_time.replace(minute=59-random.randint(0,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#even if is a simple function in case someone need to extend the code of this part can be done in an easier way\n",
    "def new_day_check (date):\n",
    "    if( date.time() == new_day_check_time.time() ):\n",
    "        randomize_start_and_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: expand this functions to generate also errors and \"attacks\"\n",
    "\n",
    "def check_plc_state(date, plc_index):\n",
    "    if(date.dayofweek in work_days and day_start_time.time() <= date.time() <= day_stop_time.time()):\n",
    "        plc_state[plc_index]=True\n",
    "    else:\n",
    "        plc_state[plc_index]=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: make the function right and add in the upper cell\n",
    "def check_plc_status_old(date, plc_index):\n",
    "\n",
    "    if(date.dayofweek in work_days):\n",
    "        if (plc_state[plc_index]==False):\n",
    "            if (plc_counter[plc_index]>=up_period):\n",
    "                plc_state[plc_index]=True\n",
    "            plc_counter += 1\n",
    "\n",
    "        elif (random.random()<=random_up_probability):\n",
    "            plc_state[plc_index]=False\n",
    "\n",
    "    elif (plc_state[plc_index]==True):\n",
    "        if (plc_counter[plc_index]>=down_for_all_day):\n",
    "            plc_state[plc_index]=False\n",
    "        plc_counter += 1\n",
    "    \n",
    "    elif (random.random()<=random_down_for_all_day_probability):\n",
    "            plc_state[plc_index]=True\n",
    "            plc_counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns names\n",
    "columns =[\"timestamp\",\"mac_address\",\"cpu_max_cycle\",\"cpu_current_cycle\",\"ram_usage\",\"rx_packets\",\"rx_bytes\",\"tx_packets\",\"tx_bytes\",\"flag\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REAL process function that generate the data\n",
    "This function get a date, devide it with a frequency that can be 10 or 1 second (manual mod), generate the rows of each subdivision, append all of them in a list, and then return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(date):\n",
    "\n",
    "    sublist = pd.date_range(date, date + timedelta(seconds=59), freq='5S').tolist()\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for subdate in sublist:\n",
    "        new_day_check(subdate)\n",
    "        for plc in range(0,len(mac_list)):\n",
    "            check_plc_state(subdate, plc)\n",
    "\n",
    "            if plc_state[plc]:\n",
    "                cpu_time = generate_cpu_cycle_time(subdate, plc)\n",
    "                ram_usage = generate_ram_usage(subdate, plc)\n",
    "                tx_packets = math.floor(random.gauss(mu_packets, sigma_packets))\n",
    "                tx_bytes = generate_tx_size(tx_packets, subdate, plc)\n",
    "                rx_packets = math.floor(random.gauss(mu_packets, sigma_packets))\n",
    "                rx_bytes = generate_rx_size(rx_packets, subdate, plc)\n",
    "                new_row = {'timestamp': subdate,\n",
    "                        'mac_address': mac_list[plc],\n",
    "                        'cpu_max_cycle': max_cycle_time[plc],\n",
    "                        'cpu_current_cycle': cpu_time,\n",
    "                        'ram_usage': ram_usage,\n",
    "                        'rx_packets': rx_packets,\n",
    "                        'rx_bytes': rx_bytes,\n",
    "                        'tx_packets': tx_packets,\n",
    "                        'tx_bytes': tx_bytes,\n",
    "                        'flag': \"ok\"}\n",
    "                result.append(new_row)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelising part\n",
    "Here we see 3 different ways to make it parallel; the second way is the fastest, but boy if nothing wanted to work at all :')\n",
    "N.B. this was my first attempt to make a python program parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a ThreadPoolExecutor for parallel processing\n",
    "max_thread_workers = 8\n",
    "max_process_workers = 8\n",
    "\n",
    "# 0=all, 1=first method, 2=second method, 3 = third method\n",
    "algorithm=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Faster than single core but a lot slower than the other 2\n",
    "It isn't really multy processor, so the speedup exist (quite a lot) but not really enough to make it viable\n",
    "~~ 37s for 31 days generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( algorithm == 0 or algorithm == 1):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_thread_workers) as executor:\n",
    "        # Process dates in parallel\n",
    "        results1 = list(executor.map(process_data, datelist))\n",
    "\n",
    "    # Concatenate the results into a single DataFrame\n",
    "    df1 = pd.concat([pd.DataFrame(result) for result in results1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fastest!\n",
    "~~ 10s for 31 days generated!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( algorithm == 0 or algorithm == 2):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_process_workers) as executor:\n",
    "        futures2 = [executor.submit(process_data, date) for date in datelist]\n",
    "        \n",
    "    # Wait for all tasks to complete\n",
    "    results2 = [future.result() for future in concurrent.futures.as_completed(futures2)]\n",
    "    # Flatten the list of lists into a single list of dictionaries\n",
    "    flat_results = [item for sublist in results2 for item in sublist]\n",
    "    # Create a DataFrame from the results\n",
    "    df2 = pd.DataFrame(flat_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### possible bottleneck in the way of recollecting the data and merging them \n",
    "This is pure speculation based on watching htop, after the full core load a single core burst to 100% (reconciliation maybe) and then goes back to all core load.\n",
    "is impossible to reduce this overhead increasing the load, like starting not only 8 thread (1 per core) but double it.\n",
    "~~ 27s for 31 days generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( algorithm == 0 or algorithm == 3):\n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_process_workers) as executor:\n",
    "        # Process dates in parallel\n",
    "        futures3 = [executor.submit(process_data, date) for date in datelist]\n",
    "        # Wait for all tasks to complete\n",
    "        concurrent.futures.wait(futures3)\n",
    "    # Collect results from completed tasks\n",
    "    results3 = [future.result() for future in futures3]\n",
    "    # Concatenate the results into a single DataFrame\n",
    "    df3 = pd.concat([pd.DataFrame(result) for result in results3], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( algorithm == 0 or algorithm == 1):\n",
    "    df1.set_index('timestamp', inplace=True)\n",
    "    print(df1.shape)\n",
    "    df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6350748, 9)\n"
     ]
    }
   ],
   "source": [
    "if ( algorithm == 0 or algorithm == 2):\n",
    "    df2.set_index('timestamp', inplace=True)\n",
    "    print(df2.shape)\n",
    "    df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ( algorithm == 0 or algorithm == 3):\n",
    "    df3.set_index('timestamp', inplace=True)\n",
    "    print(df3.shape)\n",
    "    df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (algorithm == 1):\n",
    "    df1.to_csv(\"data.csv\")\n",
    "elif (algorithm == 2):\n",
    "    df2.to_csv(\"data.csv\")\n",
    "elif (algorithm == 3):\n",
    "    df3.to_csv(\"data.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
